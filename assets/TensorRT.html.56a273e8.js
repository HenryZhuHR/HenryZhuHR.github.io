import{r as t,o,c as r,a as e,e as p,w as l,F as i,b as a,d as n}from"./app.a31c36bc.js";import{_ as c}from"./plugin-vue_export-helper.21dcd24c.js";const d={},u=a(`<h1 id="\u90E8\u7F72\u6DF1\u5EA6\u5B66\u4E60\u6A21\u578B" tabindex="-1"><a class="header-anchor" href="#\u90E8\u7F72\u6DF1\u5EA6\u5B66\u4E60\u6A21\u578B" aria-hidden="true">#</a> \u90E8\u7F72\u6DF1\u5EA6\u5B66\u4E60\u6A21\u578B</h1><p><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_example_unsupported" target="_blank" rel="noopener noreferrer">TensorRT Documentation</a></p><h2 id="\u5B89\u88C5" tabindex="-1"><a class="header-anchor" href="#\u5B89\u88C5" aria-hidden="true">#</a> \u5B89\u88C5</h2><p><a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html" target="_blank" rel="noopener noreferrer">Installing TensorRT</a></p><ul><li>\u786E\u4FDD NVIDIA CUDA\u2122 Toolkit \u5DF2\u7ECF\u5B89\u88C5\uFF1B\u652F\u6301\u7684 CUDA \u7248\u672C\u53EF\u4EE5\u5728 <a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#gettingstarted" target="_blank" rel="noopener noreferrer">Getting Started</a> \u4E2D\u627E\u5230</li><li>cuDNN \u662F\u53EF\u9009\u7684</li></ul><p>\u8FD9\u91CC\u91C7\u7528 <a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar" target="_blank" rel="noopener noreferrer">Tar File Installation</a> \u7684\u5B89\u88C5\u65B9\u5F0F\uFF0C\u56E0\u4E3A\u914D\u7F6E <code>LD_LIBRARY_PATH</code> \u5373\u53EF\uFF0C\u6BD4\u8F83\u7075\u6D3B</p><p>\u4E0B\u8F7D\u5730\u5740\uFF08\u8FDB\u5165 Get Started\uFF09 https://developer.nvidia.com/tensorrt</p><p>\u89E3\u538B\u540E\u5F97\u5230 <code>TensorRT-\${version}</code></p><div class="language-text ext-text"><pre class="language-text"><code>version=&quot;8.x.x.x&quot;
arch=$(uname -m)
cuda=&quot;cuda-x.x&quot;
tar -xzvf TensorRT-\${version}.Linux.\${arch}-gnu.\${cuda}.tar.gz
</code></pre></div><p>\u8DEF\u5F84\u6DFB\u52A0\u5230\u73AF\u5883\u53D8\u91CF <code>.bashrc</code></p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span>:<span class="token operator">&lt;</span>path-to-TensorRT-<span class="token variable">\${version}</span>/lib<span class="token operator">&gt;</span>
</code></pre></div><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> TensorRT-<span class="token variable">\${version}</span>/python
python3 -m pip <span class="token function">install</span> tensorrt-*-cp3x-none-linux_x86_64.whl
<span class="token comment"># (optinal)</span>
python3 -m pip <span class="token function">install</span> tensorrt_lean-*-cp3x-none-linux_x86_64.whl
python3 -m pip <span class="token function">install</span> tensorrt_dispatch-*-cp3x-none-linux_x86_64.whl
</code></pre></div><h2 id="onnx-\u90E8\u7F72" tabindex="-1"><a class="header-anchor" href="#onnx-\u90E8\u7F72" aria-hidden="true">#</a> ONNX \u90E8\u7F72</h2><p><a href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ex-deploy-onnx" target="_blank" rel="noopener noreferrer">Example Deployment Using ONNX</a></p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token assign-left variable">BATCH_SIZE</span><span class="token operator">=</span><span class="token number">64</span>
</code></pre></div><h2 id="\u90E8\u7F72" tabindex="-1"><a class="header-anchor" href="#\u90E8\u7F72" aria-hidden="true">#</a> \u90E8\u7F72</h2><p>\u90E8\u7F72\u5206\u4E3A\u4E24\u79CD\u65B9\u5F0F\uFF1A</p><ol><li><strong>ONNX \u90E8\u7F72</strong>\uFF1A\u5C06 pytorch \u6A21\u578B\u8F6C\u5316\u4E3A ONNX \u6A21\u578B\uFF0C\u7136\u540E\u4F7F\u7528 ONNXRuntime \u8FDB\u884C\u63A8\u7406</li><li><strong>TensorRT \u90E8\u7F72</strong>\uFF1A\uFF1A\u5C06 pytorch \u6A21\u578B\u8F6C\u5316\u4E3A ONNX/Engine \u6A21\u578B\uFF0C\u7136\u540E\u4F7F\u7528 TensorRT \u8FDB\u884C\u63A8\u7406</li></ol><p>\u8FD9\u91CC\u7ED9\u51FA\u4E00\u79CD\u63A8\u7406\u901F\u5EA6\u53C2\u8003</p><table><thead><tr><th style="text-align:center;">\u63A8\u7406\u65B9\u5F0F</th><th style="text-align:center;">\u5E73\u5747\u63A8\u7406\u65F6\u95F4</th></tr></thead><tbody><tr><td style="text-align:center;">Pytorch</td><td style="text-align:center;">30.9649 ms</td></tr><tr><td style="text-align:center;">ONNXRuntime</td><td style="text-align:center;">19.9175 ms</td></tr><tr><td style="text-align:center;">TensorRT Engine</td><td style="text-align:center;">6.5350 ms</td></tr></tbody></table><p>TensorRT \u90E8\u7F72\u5728 Jetson \u4E0A\u9700\u8981\u8003\u8651\u7CFB\u7EDF\u73AF\u5883\uFF0C\u4F8B\u5982 <a href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" target="_blank" rel="noopener noreferrer">Jetson Nano</a> \u7CFB\u7EDF\u4E2D\u5305\u542B<a href="https://developer.nvidia.com/embedded/jetpack-sdk-461" target="_blank" rel="noopener noreferrer">JetPack 4.6.1</a>:</p><ul><li><strong>OS</strong>: Ubuntu 18.04, Linux kernel 4.9</li><li><a href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-821/quick-start-guide/index.html" target="_blank" rel="noopener noreferrer"><strong>TensorRT 8.2.1</strong></a></li><li><strong>cuDNN 8.2.1</strong></li><li><a href="https://docs.nvidia.com/cuda/archive/10.2/cuda-toolkit-release-notes/index.html#title-new-features" target="_blank" rel="noopener noreferrer"><strong>CUDA 10.2</strong></a></li></ul><h3 id="_1-onnx-\u90E8\u7F72" tabindex="-1"><a class="header-anchor" href="#_1-onnx-\u90E8\u7F72" aria-hidden="true">#</a> 1. ONNX \u90E8\u7F72</h3><h3 id="_2-tensorrt-\u90E8\u7F72" tabindex="-1"><a class="header-anchor" href="#_2-tensorrt-\u90E8\u7F72" aria-hidden="true">#</a> 2. TensorRT \u90E8\u7F72</h3><blockquote><p>Ubuntu + Nvidia GPU \u73AF\u5883 \u3002Windows \u4E5F\u4E0D\u662F\u4E0D\u884C\uFF0C\u5C31\u662F\u4E0D\u60F3\u914D\u7F6E Win \u4E2D\u73AF\u5883\u53D8\u91CF\u800C\u5DF2</p></blockquote><p>\u53C2\u8003 <a href="https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8" target="_blank" rel="noopener noreferrer">TensorRT \u5B98\u65B9\u6587\u6863</a>\uFF0C\u90E8\u7F72\u6709\u4E24\u79CD\u65B9\u5F0F\uFF1A</p><ol><li>\u4F7F\u7528 TensorRT \u63A8\u7406 ONNX \u6A21\u578B</li><li>\u5C06 ONNX \u8F6C\u5316\u4E3A Engine \u6A21\u578B\u5E76\u4F7F\u7528 TensorRT \u63A8\u7406</li></ol><p>TensorRT\u5B98\u65B9\u6587\u6863 <a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html" target="_blank" rel="noopener noreferrer">&quot;NVIDIA Official Documentation&quot;</a></p><ul><li><a href="https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html" target="_blank" rel="noopener noreferrer">TensorRT Python API.</a></li><li><a href="https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/index.html" target="_blank" rel="noopener noreferrer">TensorRT C++ API.</a></li></ul><h4 id="\u5B89\u88C5-tensorrt-\u73AF\u5883" tabindex="-1"><a class="header-anchor" href="#\u5B89\u88C5-tensorrt-\u73AF\u5883" aria-hidden="true">#</a> \u5B89\u88C5 TensorRT \u73AF\u5883</h4><p>\u5FC5\u987B Ubuntu + Nvidia GPU \u73AF\u5883</p><ol><li>\u5B89\u88C5 CUDA\u3001cuDNN\u3001TensorRT\uFF0C\u53C2\u8003<a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar" target="_blank" rel="noopener noreferrer">\u5B98\u65B9\u6587\u6863</a>\uFF0C\u9700\u8981\u6CE8\u518C <a href="https://developer.nvidia.cn/login" target="_blank" rel="noopener noreferrer">Nvidia \u8D26\u53F7</a>\uFF0C\u5E76\u767B\u9646</li></ol><ul><li><p><strong>\u5B89\u88C5 CUDA</strong></p><p>\u8FDB\u5165<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener noreferrer">CUDA \u4E0B\u8F7D\u9875\u9762</a>\u5E76\u9009\u62E9\u9700\u8981\u7684\u7248\u672C\uFF0C\u6839\u636E\u7535\u8111\u7684\u914D\u7F6E\u4F9D\u6B21\u9009\u62E9\u5404\u9879\uFF0C\u6700\u540E\u9009\u62E9 <code>deb(local)</code>\u5B89\u88C5\u65B9\u5F0F\uFF0C\u5C06\u51FA\u73B0\u7684\u547D\u4EE4\u4F9D\u6B21\u590D\u5236\u5230\u7EC8\u7AEF\u4E2D\u6267\u884C\u3002</p><p><strong>\u6CE8\u610F</strong>: JetPack 4.6.1 <a href="https://developer.nvidia.com/cuda-10.2-download-archive" target="_blank" rel="noopener noreferrer">CUDA 10.2</a>\uFF0C\u4F46\u662F 10.2 \u4E0D\u652F\u6301 Ubuntu20\uFF0C\u6240\u4EE5\u4E0B\u8F7D <a href="https://developer.nvidia.com/cuda-11-4-0-download-archive" target="_blank" rel="noopener noreferrer">CUDA 11.4</a></p></li><li><p><strong>\u5B89\u88C5 cuDNN</strong></p><p>\u8FDB\u5165<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener noreferrer">cuDNN \u4E0B\u8F7D\u9875\u9762</a>\uFF0C\u6839\u636E <strong>CUDA \u7248\u672C</strong>\u548C<strong>\u7CFB\u7EDF\u67B6\u6784</strong>\u9009\u62E9\u5BF9\u5E94\u7684\u7248\u672C\u7684 <strong>Tar</strong>\u6587\u4EF6\uFF0C\u4E0B\u8F7D\u4EE5\u4E0B\u5185\u6587\u4EF6\uFF1A</p><ul><li><a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.1.32/10.2_06072021/cudnn-10.2-linux-x64-v8.2.1.32.tgz" target="_blank" rel="noopener noreferrer"><code>cuDNN v8.2.1, for CUDA 10.2, for Linux (x86)</code></a>: \u5B89\u88C5\u5728 Jetson Nano \u4E0A\u7528\u4E8E\u8F6C\u6362 Engine \u6A21\u578B</li><li><a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.1.32/11.3_06072021/cudnn-11.3-linux-x64-v8.2.1.32.tgz" target="_blank" rel="noopener noreferrer"><code>cuDNN v8.2.1, for CUDA 11.x, for Linux (x86)</code></a>: \u5B89\u88C5\u5728\u7535\u8111\u4E0A\u7528\u4E8E\u8F6C\u6362 Engine \u6A21\u578B\u548C\u6D4B\u8BD5\uFF0C\u8FD9\u4E00\u6B65\u662F\u4E3A\u4E86\u5728\u7535\u8111\u4E0A\u7F16\u5199\u63A8\u7406\u4EE3\u7801\u548C\u6D4B\u8BD5\uFF0C\u5982\u679C\u63A8\u7406\u4EE3\u7801\u5DF2\u7ECF\u5199\u597D\uFF0C\u53EF\u4EE5\u4E0D\u7528\u5B89\u88C5</li><li><a href="https://developer.nvidia.com/downloads/compute/cudnn/secure/8.9.0/local_installers/11.8/cudnn-linux-x86_64-8.9.0.131_cuda11-archive.tar.xz/" target="_blank" rel="noopener noreferrer"><code>cuDNN8.9.0 + CUDA 11.8 + Linux x86_64 (Tar)</code></a>: \u6700\u65B0\u7248\u672C</li></ul><p>\u4E0B\u8F7D\u540E\u5F97\u5230\u538B\u7F29\u5305 <code>cudnn-\${version}.tar.xz</code> \uFF0C\u5C06\u538B\u7F29\u5305\u89E3\u538B\u540E\u5F97\u5230\u540C\u540D\u7684\u76EE\u5F55\uFF0C\u4F46\u662F8.2.1 \u89E3\u538B\u540E\u5F97\u5230\u7684\u662F cuda \u76EE\u5F55\uFF0C\u5EFA\u8BAE\u6539\u540D</p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token function">tar</span> -xf cudnn-<span class="token variable">\${version}</span>.tar.xz
</code></pre></div></li><li><p><strong>\u5B89\u88C5 TensorRT</strong></p><p>\u8FDB\u5165<a href="https://developer.nvidia.com/tensorrt-getting-started" target="_blank" rel="noopener noreferrer">\u4E0B\u8F7D\u9875\u9762</a>\uFF0C\u70B9\u51FB <em>Download Now</em> \u7684\u5165\u53E3\uFF0C\u9009\u62E9\u7248\u672C\u7684 Tar \u6587\u4EF6\u4E0B\u8F7D\uFF1A</p><ul><li><a href="https://developer.nvidia.com/compute/machine-learning/tensorrt/secure/8.2.1/tars/tensorrt-8.2.1.8.linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz" target="_blank" rel="noopener noreferrer"><code>TensorRT 8.2 GA for Linux x86_64 and CUDA 11.0-5 TAR Package</code></a></li></ul><p>\u4E0B\u8F7D\u540E\u5F97\u5230\u538B\u7F29\u5305 <code>TensorRT-\${version}.tar.gz</code> \uFF0C\u5C06\u538B\u7F29\u5305\u89E3\u538B\u540E\u5F97\u5230\u540C\u540D\u7684\u76EE\u5F55</p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token function">tar</span> -xf TensorRT-<span class="token variable">\${version}</span>.tar.gz
</code></pre></div></li></ul><p>\u5728\u7CFB\u7EDF\u73AF\u5883\u53D8\u91CF\u4E2D\u6DFB\u52A0 CUDA, cuDNN,TensorRT \u76F8\u5173\u7684\u8DEF\u5F84(\u4FEE\u6539\u4E3A\u771F\u5B9E\u8DEF\u5F84<code>$xxx_HOME</code>)\u3002\u6DFB\u52A0\u5B8C\u6210\u540E\uFF0C<code>source ~/.bashrc</code> \u4F7F\u73AF\u5883\u53D8\u91CF\u751F\u6548</p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token comment"># ------ CUDA ------</span>
<span class="token assign-left variable">CUDA_VERSION</span><span class="token operator">=</span><span class="token number">11.8</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda-<span class="token variable">\${CUDA_VERSION}</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/lib64
<span class="token comment"># ------ cuDNN 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDNN_HOME</span><span class="token operator">=</span>path/to/cudnn-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span> 
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDNN_HOME</span>/lib64 <span class="token comment"># \u65B0\u7248\u672C\u4E3A lib \u76EE\u5F55\uFF0C\u5EFA\u8BAE\u81EA\u5DF1\u68C0\u67E5\u4E00\u4E0B</span>
<span class="token comment"># ------ TensorRT 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TENSORRT_HOME</span><span class="token operator">=</span>path/to/TensorRT-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/lib
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/bin
</code></pre></div><ol start="2"><li>\u5B89\u88C5 python\u76F8\u5173\u73AF\u5883</li></ol><p>\u53C2\u8003\u5B98\u65B9\u6587\u6863 <a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip" target="_blank" rel="noopener noreferrer">&quot;Python Package Index Installation&quot;</a> \u5B89\u88C5 python \u7684<a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip" target="_blank" rel="noopener noreferrer"><code>tensorrt</code></a></p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token comment"># \u6839\u636E python \u7248\u672C\u9009\u62E9\u5BF9\u5E94\u7684 .whl \u6587\u4EF6</span>
python3 -m pip <span class="token function">install</span> --upgrade <span class="token variable">$TENSORRT_HOME</span>/python/tensorrt-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span>.whl
python3 -m pip <span class="token function">install</span> --upgrade pycuda<span class="token operator">&gt;=</span><span class="token number">2020.1</span> <span class="token comment"># \u5DF2\u5199\u5165 requirements.txt</span>
</code></pre></div><h4 id="_1-tensorrt-\u63A8\u7406-onnx-\u6A21\u578B" tabindex="-1"><a class="header-anchor" href="#_1-tensorrt-\u63A8\u7406-onnx-\u6A21\u578B" aria-hidden="true">#</a> 1. TensorRT \u63A8\u7406 ONNX \u6A21\u578B</h4><p>\u53C2\u8003\u5B98\u65B9<a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_topics" target="_blank" rel="noopener noreferrer">Python API</a></p><div class="language-bash ext-sh"><pre class="language-bash"><code>python3 -m pip <span class="token function">install</span> --upgrade pip
</code></pre></div><h4 id="_2-tensorrt-\u63A8\u7406-engine-\u6A21\u578B" tabindex="-1"><a class="header-anchor" href="#_2-tensorrt-\u63A8\u7406-engine-\u6A21\u578B" aria-hidden="true">#</a> 2. TensorRT \u63A8\u7406 Engine \u6A21\u578B</h4><p>\u53C2\u8003\u5B98\u65B9\u6587\u6863<a href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-821/quick-start-guide/index.html#export-from-pytorch" target="_blank" rel="noopener noreferrer">&quot;Exporting To ONNX From PyTorch&quot;/&quot;Converting ONNX To A TensorRT Engine&quot;</a>\u8FD9\u79CD\u65B9\u5F0F\u7684\u90E8\u7F72\u6D41\u7A0B\u662F\uFF1A\u9996\u5148\u5C06 ONNX \u6A21\u578B\u8F6C\u5316\u4E3A TensorRT Engine \u6A21\u578B\uFF0C\u518D\u4F7F\u7528 TensorRT API \u63A8\u7406 Engine \u6A21\u578B</p><p>\u8F6C\u6362\u8FC7\u7A0B\u4E2D\u9700\u8981\u8003\u8651\u5230 TensorRT \u652F\u6301\u7684 ONNX \u7248\u672C\uFF0C\u5177\u4F53\u53EF\u4EE5\u53C2\u8003 <a href="https://github.com/NVIDIA/TensorRT/tree/release/8.2" target="_blank" rel="noopener noreferrer">TensorRT(8.2.1) \u6E90\u7801</a>\u4E2D\u7684 <a href="https://github.com/NVIDIA/TensorRT/blob/release/8.2/samples/python/efficientnet/requirements.txt" target="_blank" rel="noopener noreferrer"><code>requirements.txt</code></a> \u6587\u4EF6\uFF0C\u6240\u4EE5\u9700\u8981\u91CD\u65B0\u5B89\u88C5 ONNX \u518D\u91CD\u65B0\u8F6C\u6362</p><div class="language-bash ext-sh"><pre class="language-bash"><code>python3 -m pip <span class="token function">install</span> <span class="token assign-left variable">onnx</span><span class="token operator">==</span><span class="token number">1.9</span>.0 <span class="token comment"># \u5DF2\u5199\u5165 requirements.txt</span>
python3 export.py temp/culane.py
</code></pre></div><p>\u5728\u8F6C\u6362\u4E4B\u524D\uFF0C\u786E\u4FDD\u6709 <code>**-INT32.onnx</code> \u6A21\u578B\uFF0C\u56E0\u4E3A TensorRT \u652F\u6301 INT32 \u800C\u4E0D\u652F\u6301 INT64\u3002\u5982\u679C\u6CA1\u6709\uFF0C\u53EF\u4EE5\u4F7F\u7528 <code>onnxsim</code> \u5DE5\u5177\u8FDB\u884C\u8F6C\u6362</p><div class="language-bash ext-sh"><pre class="language-bash"><code>python3 -m onnxsim weights/culane_18.onnx weights/culane_18-sim.onnx
</code></pre></div><p>\u4E0A\u8FF0\u64CD\u4F5C\u90FD\u53EF\u4EE5\u5728\u7535\u8111\u4E0A\u5B8C\u6210\uFF0C\u4F46\u662F\u5982\u679C\u9700\u8981\u5728 Jetson Nano \u4E0A\u63A8\u7406\uFF0C\u5219\u9700\u8981\u5C06 onnx \u8F6C\u6362\u4E3A Engine \u6A21\u578B\u9700\u8981\u5728 Jetson Nano \u4E0A\u5B8C\u6210\uFF0C\u5426\u5219\u4F1A\u51FA\u73B0<a href="#issuse-tensorrt-engine_incompatible_device">&quot;<em>\u4E0D\u5339\u914D\u8BBE\u5907\u7684\u62A5\u9519</em>&quot;</a>\uFF0C\u4F46\u662F\u540E\u9762\u7684\u6B65\u9AA4\u53EF\u4EE5\u5728\u7535\u8111\u4E0A\u6D4B\u8BD5\u6CA1\u6709\u95EE\u9898\u518D\u5728 Jetson Nano \u4E0A\u90E8\u7F72\u3002</p><p>Jetpack \u81EA\u5E26\u7684\u5E93\u5728 <code>/usr/src</code>\uFF0C\u56E0\u6B64\u5728 Jetson Nano \u7684\u7CFB\u7EDF\u73AF\u5883\u53D8\u91CF\u4E2D\u6DFB\u52A0\u5982\u4E0B\u5185\u5BB9\uFF0C\u7136\u540E<code>source ~/.bashrc</code> \u4F7F\u73AF\u5883\u53D8\u91CF\u751F\u6548</p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token comment"># ------ CUDA 10.2 ------</span>
<span class="token assign-left variable">CUDA_VERSION</span><span class="token operator">=</span><span class="token number">10.2</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda-<span class="token variable">\${CUDA_VERSION}</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/lib64
<span class="token comment"># ------ TensorRT 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TENSORRT_HOME</span><span class="token operator">=</span>/usr/src/tensorrt
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/lib
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/bin
</code></pre></div>`,50),h=n("\u4F7F\u7528\u5B98\u65B9\u63D0\u4F9B\u7684\u8F6C\u6362\u5DE5\u5177\u8FDB\u884C\u8F6C\u6362\uFF0C\u5982\u679C TensorRT \u73AF\u5883\u914D\u7F6E\u6B63\u786E\uFF0C\u5C06 "),g=e("code",null,"**-INT32.onnx",-1),k=n(" \u8F6C\u5316\u4E3A "),b=e("code",null,"**-INT32.engine",-1),m=n("\u3002\u8F6C\u6362\u8FC7\u7A0B\u4E2D\u53EF\u80FD\u51FA\u73B0\u7684bug\u4EE5\u53CA\u89E3\u51B3\u65B9\u6848\u8BB0\u5F55\u5728"),v=e("a",{href:"#TensorRT-Engine-%E8%BD%AC%E6%8D%A2%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98"},'"Tensor RT Engine \u8F6C\u6362\u8FC7\u7A0B\u4E2D\u7684\u95EE\u9898"',-1),T=n("\u4E2D\uFF0C\u8FD9\u91CC\u63D0\u4F9B\u4E00\u4E2A"),f=n("\u8F93\u51FA\u7EC6\u8282\u53C2\u8003"),x=a(`<div class="language-bash ext-sh"><pre class="language-bash"><code>python3 export.py configs/culane.py
trtexec --verbose --fp16 <span class="token punctuation">\\</span>
  --onnx<span class="token operator">=</span>weights/culane_18-INT32.onnx <span class="token punctuation">\\</span>
  --saveEngine<span class="token operator">=</span>weights/culane_18-INT32.engine
</code></pre></div><ul><li><code>--workspace=N</code>: Set workspace size in megabytes (default = 16)</li><li><code>--fp16</code>: Enable fp16 precision, in addition to fp32 (default = disabled)</li><li><code>--int8</code>: Enable int8 precision, in addition to fp32 (default = disabled)</li><li><code>--verbose</code>: Use verbose logging (default = false)</li><li><code>--exportTimes=&lt;file&gt;</code> :Write the timing results in a json file (default = disabled)</li><li><code>--exportOutput=&lt;file&gt;</code>: Write the output tensors to a json file (default = disabled)</li><li><code>--exportProfile=&lt;file&gt;</code>: Write the profile information per layer in a json file (default = disabled)</li></ul><blockquote><p>\u5B9E\u9645\u4E0A\uFF0CNV \u5B98\u65B9\u63D0\u4F9B\u4E86 <a href="https://github.com/NVIDIA-AI-IOT/torch2trt" target="_blank" rel="noopener noreferrer"><code>torch2trt</code></a> \u8F6C\u6362\u5DE5\u5177\u53EF\u4EE5\u76F4\u63A5\u5B8C\u6210 <code>PyTroch -&gt; Engine</code>\uFF0C\u4F46\u662F\u5728 Jetson Nano \u4E0A\u90E8\u7F72\u7684\u65F6\u5019\uFF0C\u9700\u8981\u5728 Jetson Nano \u4E0A\u5B8C\u6210\u6A21\u578B\u8F6C\u6362\uFF0C\u5426\u5219\u5728\u5B9E\u9645\u4F7F\u7528\u65F6\u4F1A\u51FA\u73B0<a href="#issuse-tensorrt-engine_incompatible_device">\u4E0D\u5339\u914D\u8BBE\u5907\u7684\u62A5\u9519</a>\uFF0C\u90A3\u4E48\u5C06\u5B8C\u6574\u7684 torch \u9879\u76EE\u76F4\u63A5\u5B89\u88C5\u5728 Jetson Nano \u4E0A\u53EF\u80FD\u4F1A\u9047\u5230\u5F88\u591A\u95EE\u9898\uFF0C\u56E0\u6B64\uFF0C\u8FD9\u91CC\u5148\u5728\u7535\u8111\u4E0A\u5B8C\u6210 <code>PyTroch -&gt; ONNX</code> \u8F6C\u6362\uFF0C\u7136\u540E\u5C06\u8F6C\u6362\u597D\u7684 <code>ONNX</code> \u590D\u5236\u5230 Jetson Nano \u4E0A\u4F7F\u7528 <code>trtexec</code> \u5B8C\u6210 <code>ONNX -&gt; Engine</code> \u8F6C\u6362\uFF0C\u53EF\u4EE5\u907F\u514D\u5728 Jetson Nano \u4E0A\u5B89\u88C5 torch \u73AF\u5883\u548C\u9879\u76EE\u7684\u4E00\u4E9B\u5176\u4ED6\u73AF\u5883\u3002</p></blockquote><p>\u8FD0\u884C\u524D\u9700\u8981\u5B89\u88C5 <code>pycuda</code></p><div class="language-bash ext-sh"><pre class="language-bash"><code>python3 -m pip <span class="token function">install</span> pycuda<span class="token operator">&gt;=</span><span class="token number">2020.1</span>
</code></pre></div><p>\u7136\u540E\u4FEE\u6539 <code>deploy/infer-trtEngine.py</code> \u4E2D\u7684 <code>TRT_MODEL_PATH</code> \u4E3A <code>**-INT32.engine</code> \u7684\u8DEF\u5F84\uFF0C\u8FD0\u884C</p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> deploy
python3 infer-trtEngine.py
</code></pre></div><h2 id="\u95EE\u9898\u4E0E\u89E3\u51B3\u65B9\u6848" tabindex="-1"><a class="header-anchor" href="#\u95EE\u9898\u4E0E\u89E3\u51B3\u65B9\u6848" aria-hidden="true">#</a> \u95EE\u9898\u4E0E\u89E3\u51B3\u65B9\u6848</h2><h3 id="tensor-rt-engine-\u8F6C\u6362\u8FC7\u7A0B\u4E2D\u7684\u95EE\u9898" tabindex="-1"><a class="header-anchor" href="#tensor-rt-engine-\u8F6C\u6362\u8FC7\u7A0B\u4E2D\u7684\u95EE\u9898" aria-hidden="true">#</a> Tensor RT Engine \u8F6C\u6362\u8FC7\u7A0B\u4E2D\u7684\u95EE\u9898</h3><ul><li><p>\u62A5\u9519\u5982\u4E0B</p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token punctuation">[</span>07/13/2023-11:01:12<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> Error<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>: <span class="token punctuation">[</span>caskUtils.cpp::trtSmToCask::147<span class="token punctuation">]</span> Error Code <span class="token number">1</span>: Internal Error <span class="token punctuation">(</span>Unsupported SM: 0x809<span class="token punctuation">)</span>
<span class="token punctuation">[</span>07/13/2023-11:01:12<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> Error<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>: <span class="token punctuation">[</span>builder.cpp::buildSerializedNetwork::609<span class="token punctuation">]</span> Error Code <span class="token number">2</span>: Internal Error <span class="token punctuation">(</span>Assertion enginePtr <span class="token operator">!=</span> nullptr failed. <span class="token punctuation">)</span>
</code></pre></div><p>\u53C2\u8003 <a href="https://github.com/NVIDIA/TensorRT/issues/2727#issuecomment-1492809565" target="_blank" rel="noopener noreferrer">TensorRT #2727</a>\uFF0C<code>Unsupported SM</code> \u8868\u793A\u8BE5\u7248\u672C\u7684 TensorRT \u4E0D\u652F\u6301\u5F53\u524D\u7684 GPU \u7684 SM\uFF08SM\u662F\u6D41\u5A92\u4F53\u591A\u5904\u7406\u5668(Streaming Multiprocessor)\uFF0CRTX40\u7CFB\u5217\u5177\u6709\u4E0E\u4EE5\u524D\u7684GPU\u7CFB\u5217\u4E0D\u540C\u7684SM\u67B6\u6784\uFF09\uFF0C\u9700\u8981\u5347\u7EA7 TensorRT \u7248\u672C\uFF0C\u6216\u8005\u9009\u62E9\u6BD4\u5982RTX3080\u7684GPU\uFF0CTensorRT 8.5.1.7\u4EE5\u4E0A\u7248\u672C\u652F\u6301RTX40\u7CFBSM</p></li><li><p>\u62A5\u9519\u5982\u4E0B<span id="issuse-tensorrt-engine_incompatible_device"></span></p><div class="language-bash ext-sh"><pre class="language-bash"><code><span class="token punctuation">[</span>07/14/2023-11:41:43<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> <span class="token number">6</span>: The engine plan <span class="token function">file</span> is generated on an incompatible device, expecting compute <span class="token number">5.3</span> got compute <span class="token number">6.1</span>, please rebuild.
<span class="token punctuation">[</span>07/14/2023-11:41:43<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> <span class="token number">4</span>: <span class="token punctuation">[</span>runtime.cpp::deserializeCudaEngine::50<span class="token punctuation">]</span> Error Code <span class="token number">4</span>: Internal Error <span class="token punctuation">(</span>Engine deserialization failed.<span class="token punctuation">)</span>
</code></pre></div><p>\u8FD9\u662F\u7531\u4E8E Engine \u6A21\u578B\u4E0D\u662F\u5728 Jetson nano \u4E0A\u751F\u6210\u7684\uFF0C\u5728 Jetson nano \u4E0A\u91CD\u65B0\u751F\u6210 Engine \u6A21\u578B\u5373\u53EF</p></li></ul>`,10);function _(N,R){const s=t("RouterLink");return o(),r(i,null,[u,e("p",null,[h,g,k,b,m,v,T,p(s,{to:"/program/Nvidia/TensorRT/docs/onnx2engine.html"},{default:l(()=>[f]),_:1})]),x],64)}var D=c(d,[["render",_]]);export{D as default};
