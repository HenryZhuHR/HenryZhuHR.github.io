import{_ as e,c as a,o as i,a2 as t}from"./chunks/framework.bdxXucLJ.js";const f=JSON.parse('{"title":"目标检测算法学习记录","description":"","frontmatter":{"outline":"deep","title":"目标检测算法学习记录"},"headers":[],"relativePath":"machine_learning/object_detection/object_detection.md","filePath":"machine_learning/object_detection/object_detection.md","lastUpdated":1715879691000}'),n={name:"machine_learning/object_detection/object_detection.md"},o=t('<h1 id="mu-biao-jian-ce-suan-fa-xue-xi-ji-lu" tabindex="-1">目标检测算法学习记录 <a class="header-anchor" href="#mu-biao-jian-ce-suan-fa-xue-xi-ji-lu" aria-label="Permalink to &quot;目标检测算法学习记录&quot;">​</a></h1><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><h3 id="mu-biao-jian-ce-ren-wu-de-ding-yi" tabindex="-1">目标检测任务的定义 <a class="header-anchor" href="#mu-biao-jian-ce-ren-wu-de-ding-yi" aria-label="Permalink to &quot;目标检测任务的定义&quot;">​</a></h3><p><strong>目标检测 (Object Detection)</strong> 是计算机视觉三大任务之一，其目的是在给定的图像中检测出图像中的目标物体，并对其进行分类和定位</p><h3 id="mu-biao-jian-ce-suan-fa-de-fa-zhan" tabindex="-1">目标检测算法的发展 <a class="header-anchor" href="#mu-biao-jian-ce-suan-fa-de-fa-zhan" aria-label="Permalink to &quot;目标检测算法的发展&quot;">​</a></h3><h2 id="jing-dian-suan-fa" tabindex="-1">经典算法 <a class="header-anchor" href="#jing-dian-suan-fa" aria-label="Permalink to &quot;经典算法&quot;">​</a></h2><h3 id="r-cnn-xi-lie" tabindex="-1">R-CNN 系列 <a class="header-anchor" href="#r-cnn-xi-lie" aria-label="Permalink to &quot;R-CNN 系列&quot;">​</a></h3><h3 id="yolo-xi-lie" tabindex="-1">YOLO 系列 <a class="header-anchor" href="#yolo-xi-lie" aria-label="Permalink to &quot;YOLO 系列&quot;">​</a></h3><h3 id="yolov4" tabindex="-1">YOLOv4 <a class="header-anchor" href="#yolov4" aria-label="Permalink to &quot;YOLOv4&quot;">​</a></h3><h2 id="shi-pin-liu-mu-biao-jian-ce" tabindex="-1">视频流目标检测 <a class="header-anchor" href="#shi-pin-liu-mu-biao-jian-ce" aria-label="Permalink to &quot;视频流目标检测&quot;">​</a></h2><p>相比于单帧检测，视频目标检测多了 <strong>Temporal Context</strong> (时间上下文信息)，充分利用好时序上下文关系，可以减少视频中连续帧之间的大量冗余信息，提高检测速度；还可以解决视频相对于图像存在的运动模糊、视频失焦、部分遮挡以及奇异姿势等问题，还可以提高检测质量， <sup><a href="#ref-zhihu_q52185576_a155679253">[1]</a>, <a href="#ref-zhihu_q52185576_a413306776">[2]</a></sup></p><h2 id="can-kao-zi-liao-he-wen-xian" tabindex="-1">参考资料和文献 <a class="header-anchor" href="#can-kao-zi-liao-he-wen-xian" aria-label="Permalink to &quot;参考资料和文献&quot;">​</a></h2><h3 id="can-kao-zi-liao" tabindex="-1">参考资料 <a class="header-anchor" href="#can-kao-zi-liao" aria-label="Permalink to &quot;参考资料&quot;">​</a></h3><ul><li><p>[1] <span id="ref-zhihu_q52185576_a155679253"></span> <a href="https://www.zhihu.com/question/52185576/answer/155679253" target="_blank" rel="noreferrer">视频中的目标检测与图像中的目标检测具体有什么区别？ - Naiyan Wang的回答 - 知乎</a></p></li><li><p>[2] <span id="ref-zhihu_q52185576_a413306776"></span> <a href="https://www.zhihu.com/question/52185576/answer/413306776" target="_blank" rel="noreferrer">视频中的目标检测与图像中的目标检测具体有什么区别？ - 亦辰的回答 - 知乎</a></p></li><li><p><a href="https://github.com/guanfuchen/video_obj" target="_blank" rel="noreferrer">guanfuchen/video_obj: 基于视频的目标检测算法研究(华为媒体研究院)</a></p></li></ul><h3 id="can-kao-wen-xian" tabindex="-1">参考文献 <a class="header-anchor" href="#can-kao-wen-xian" aria-label="Permalink to &quot;参考文献&quot;">​</a></h3><blockquote><p>literature</p></blockquote><ul><li>[l-1] <span id="literature-zhihu_q52185576_a155679253"></span> <a href="https://www.zhihu.com/question/52185576/answer/155679253" target="_blank" rel="noreferrer">视频中的目标检测与图像中的目标检测具体有什么区别？ - Naiyan Wang的回答 - 知乎</a></li><li></li></ul><h2 id="wei-zheng-li-nei-rong" tabindex="-1">未整理内容 <a class="header-anchor" href="#wei-zheng-li-nei-rong" aria-label="Permalink to &quot;未整理内容&quot;">​</a></h2><blockquote><p>简单来说，视频检测是比单张图片检测多了Temporal Context（时间上下文）的信息。不同方法想利用这些Context来解决的问题并不相同。一类方法是关注如何使用这部分信息来<strong>加速Video Detection</strong>。因为相邻帧之间存在大量冗余，如果可以通过一些廉价的办法来加速不损害性能，在实际应用中还是很有意义的。另一类方法是关注这部分信息可以有效<strong>减轻单帧图片检测中由于运动模糊，物体面积过小导致的困难</strong>，从而来提升性能。</p><ol><li>CUHK: Xiaogang Wang 这面我了解到的有三篇文章，最开始 (TPAMI Short)是通过Motion的信息以及多类之间的Correlation来对单帧图像detector的输出进行后处理，算是在前面提到的Baseline方法上的小改进。后续的文章(CVPR 16)在这个基础上，引入了一个Temporal CNN对每一个Tubelet进行rescore。这样通过Temporal的信息来重新评估每个proposal的置信度。最近的工作(CVPR17)将Proposal生成这个步骤，也从静态图片拿到了时序上来做。除此之外，对于每个Tubelet的分类，也采取了流行的LSTM。</li><li>MSRA: Jifeng Dai 相对来讲，这面的工作更干净，思路更清晰一些。个人来说更喜欢。这面的两个工作其实思想类似，但是恰好对应于前文提到的加速和性能提升两个目的。其核心都在于通过快速计算Optical Flow来捕捉视频中的Motion信息，然后通过这个Flow的信息使用Bilinear Sampling对之前的Feature Map进行Warp（也就是通过Optical Flow来预测当前帧的Feature Map）。有了这样的信息之后，如果我们想加速，那么可以直接使用预测的Feature Map来输出结果；如果想得到更好的结果，可以将预测的Feature Map和当前帧计算出来的Feature Map融合起来一起输出结果。值得一提的是，后者也是目前唯一一个End to End的Video Detection方法。另外有一些零碎一些的工作，基本都是在后处理过程中，处理rescore detection的问题，例如Seq-NMS等等。 作者：Naiyan Wang 链接：<a href="https://www.zhihu.com/question/52185576/answer/155679253" target="_blank" rel="noreferrer">https://www.zhihu.com/question/52185576/answer/155679253</a> 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</li></ol></blockquote><p><strong>单帧不够，多帧来凑</strong></p><h3 id="shi-pin-mu-biao-jian-ce-de-yi-yi" tabindex="-1">视频目标检测的意义 <a class="header-anchor" href="#shi-pin-mu-biao-jian-ce-de-yi-yi" aria-label="Permalink to &quot;视频目标检测的意义&quot;">​</a></h3><p>传统的基于图片的目标检测方法已经非常成熟，对于视频目标检测来说，如果视频流按帧一张一张使用图片的目标检测算法来处理会出现以下两类问题：</p><ul><li>因为视频流的图片信息具有时间和空间相关性，相邻帧之间的特征提取网络会输出有冗余的特征图信息，会造成没必要的计算浪费。</li><li>图片的目标检测算法在目标物体运动模糊，拍摄焦距失调，物体部分遮挡，非刚性物体罕见变形姿态的情况下，很难获得较为准确的结果，而这些情况（如下图）在视频的拍摄中情况较为多见。</li></ul><blockquote><p>上述意义引用自<a href="https://zhuanlan.zhihu.com/p/37068429" target="_blank" rel="noreferrer">Towards High Performance Video Object Detection论文笔记</a>，具体内容参考该网址。</p></blockquote><p>对于自动驾驶来说语义分割是自动驾驶中理解周围环境的一项常见任务。可行驶区域分割和车道检测对于道路上安全且高效的导航尤为重要。为了满足自动驾驶汽车中可行驶区域和车道分割的高效轻量级，《<a href="https://arxiv.org/pdf/2307.10705.pdf" target="_blank" rel="noreferrer">TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars</a>》提出了一种用于可行驶区域和车道线分割的轻量级模型。TwinLiteNet设计代价低廉，但是可以获得精确且高效的分割结果。TwinLiteNet可以在计算能力有效的嵌入式设备上实时地运行，尤其是因为它在Jetson Xavier NX上实现了60FPS帧率，这使其成为自动驾驶汽车的理想解决方案。具体的开源方案已经在Github上实现了。原文链接：<a href="https://blog.csdn.net/lovely_yoshino/article/details/132018561" target="_blank" rel="noreferrer">https://blog.csdn.net/lovely_yoshino/article/details/132018561</a></p>',25),r=[o];function l(h,c,s,u,d,p){return i(),a("div",null,r)}const _=e(n,[["render",l]]);export{f as __pageData,_ as default};
